{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Competition \n",
    "\n",
    "## Group name : Maverick\n",
    "## Student Names : \n",
    "\n",
    "\n",
    "- Aishwarya Inamdar        : \n",
    "19084404    (100%contribution)\n",
    "- Fasahat Khan             : \n",
    "19121474    (100%contribution)\n",
    "- Kumar Ankitesh           : \n",
    "19059426    (100%contribution)\n",
    "- Mayank Rikh              : \n",
    "19018797    (100%contribution)\n",
    "- Parna Salian             : \n",
    "19111002    (100%contribution)\n",
    "\n",
    "### Summary:\n",
    "We have implemented a variety of machine learning models to classify whether a software has a bug or not in classification dataset. The highest test accuracy is 99%, achieved by Random Forest with fine-tuned parameters. Feature selections were used to reduce model complexity and overfitting, and improved test accuracies were observed. Because the dataset consists of 22 labels and that the test accuracy of XGBoost model is 98%, we have built successful models for identifying bugs in a software. As Random Forest classifier comprises of decision trees on the randomly selected data samples, it gets the prediction from each tree and selects the best solution by means of voting. The method used in Random Forest is based on divide-and-conquer approach of decision trees generated on randomly split dataset. Finally, it is simpler and more powerful as compared to other non-linear classification algorithms. To further improve the accuracy, we need more data to train our model. Looking forward, there are several extensions to this work that could be done as stated below:\n",
    "- Dig deeper into multinomialNB to find way to solve its overfitting problem; \n",
    "- Gathering more data to balance data distribution; \n",
    "- Model stacking: further combining classifiers to improve performance. Also Bagging can be done to decrease the model variance - Real application: input new data for software and transform them into features the same way as we mentioned and apply our machine learning models to predict bugs.\n",
    " \n",
    "### Libraries Used: pandas, numpy and sklearn\n",
    "In the algorithms used, we are passing the train data set (X) along with its output (y) and the test data set to test the accuracy of the model. We have defined a range of values from -2 to 3 and creating a set of values in variable ‘r’ using Pow() function which are used as parameters (params) stored in Param library. \n",
    "\n",
    "### User defined functions: \n",
    "\n",
    "- generateCSV(): \n",
    "Generates the final result in CSV format containing the corresponding unique IDs of different features of the     testDataset and respective probability.\n",
    "- calculateAccuracy(): \n",
    "The RepeatedKFold() function splits the data set in 3 parts and the cross-validator is repeated 5 times. cross_val_score() gives the score which determines the accuracy based on number of folds ‘kf’\n",
    "- mle(): \n",
    "to find the maximum likelihood estimator.\n",
    "- bestParamCalculator():  \n",
    "to evaluate the predictions on the data set based on negated mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1: Importing the requirements and ignoring warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.svm import SVC\n",
    "from pandas import DataFrame\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from numpy import sort\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "\n",
    "#dictionary containing various scores\n",
    "resultsDict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Utility method definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Method - generateCSV()\n",
    "- Params - probability , testDataset\n",
    "- Description - This function generates the csv file based on testDataset and probability using data frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auxilary methods\n",
    "def generateCSV(probability, testDataset):\n",
    "    print()\n",
    "    print('************ CSV FILE ***************')\n",
    "    print()\n",
    "\n",
    "    idValues = testDataset['Id']\n",
    "    finalDataset = {\n",
    "        'Id' : idValues,\n",
    "        'Category' : probability\n",
    "    }\n",
    "\n",
    "    df = DataFrame(finalDataset, columns = ['Id', 'Category'])\n",
    "    df.to_csv (r'export_dataframe.csv', index = None, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Maximum Likelihood Estimator\n",
    "Maximum Likelihood estimation selects the parameter values that makes the observed data most probable. The below mle function takes in the output of train dataset as the parameter and calculates the maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(y):\n",
    "    print()\n",
    "    print('************** MAXIMUM LIKELIHOOD ***************')\n",
    "    print()\n",
    "    maximumLikelihoodEstimate = y.value_counts(normalize = 1)\n",
    "    print('Maximum likelihood ', maximumLikelihoodEstimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### bestParamCalculator()\n",
    "bestParamCalculator method takes in the train data, output of train dataset, model, parameters, boolean values for data pre-processing algorithms and with the help of RandomizedSearchCV function, gets the best model of all the possible permutations of the passed parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestParamCalculator(X, y, model, params, applyScalar = False, applyNormalizer = False):\n",
    "\n",
    "    kf = RepeatedKFold(n_splits = 3, n_repeats = 5, random_state = None) \n",
    "    random = RandomizedSearchCV(estimator = model, param_distributions = params, cv = kf, verbose = 2, random_state = 42, n_jobs = -1, scoring='neg_mean_squared_error')\n",
    "    if applyScalar:\n",
    "        scaler = RobustScaler().fit(X)\n",
    "        X = scaler.transform(X)   \n",
    "\n",
    "    if applyNormalizer:\n",
    "        normalizer = Normalizer().fit(X)\n",
    "        X = normalizer.transform(X)\n",
    "\n",
    "    random.fit(X, y)\n",
    "    return random.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy and Confusion Matrix for each repeated K fold\n",
    "calculateAccuracyAndConfusion method performs RepeatedKFold cross evaluation on the model to result out the best accuracy and prints in the console along with the confusion matrix. This method takes in the model, train dataset, output of train dataset and name of the model as a string so that we can do a boxplot comparison of all the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracyAndConfusion(model, X, y, name = \"\"):\n",
    "    print()\n",
    "    print('****************** ACCURACY *******************')\n",
    "    print()\n",
    "\n",
    "    kf = RepeatedKFold(n_splits = 3, n_repeats = 5, random_state = None) \n",
    "    results = cross_val_score(model, X, y, cv=kf)\n",
    "    resultsDict[name] = results\n",
    "    #Accuracy measure \n",
    "    print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean() * 100.0, results.std() * 100.0))\n",
    "\n",
    "    print()\n",
    "    print('****************** CONFUSION MATRIX FOR EACH REPEATED K FOLD *******************')\n",
    "    print()\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        print(confusion_matrix(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot of comparison of performance beyween various algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPerformancePlot():\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithms Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(list(resultsDict.values()))\n",
    "    ax.set_xticklabels(list(resultsDict.keys()))\n",
    "    plt.savefig('performance.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: ML algorithms covered in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is used when a classification has to be performed on the dependent variables. For example, to predict if an email is spam or not spam and if a child is happy or sad. The method logistic() takes Train dataset,  output of train dataset, test datasetand boolean value for preprocessing. In the implementation, a range of values from -2 to 3 is stored in the variable 'n' and variable 'r' uses a pow function on the this range for values of 'n' to give us a range of C values for predicting the accuracy for the best hyperparameter value. Using Repeated K Fold function, maximum accuracy is calculated and its corresponding C value is estimated. RobustScalar function is used for data preprocessing.\n",
    "\n",
    "##### Parameters: \n",
    "- C: It is the inverse of regularization strength i.e. lambda. Higher the value of C, more will be the complexity and vice versa. In other words, smaller value of C means higher level of smoothening and greater values of C means lesser smoothening.\n",
    "\n",
    "##### Drawbacks : \n",
    "- Continuous outcome cannot be predicted from logistic regression.\n",
    "- It is mandatory that all the data points should be independent of other data points. If they are dependent on each other, then the model will tend to overweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, y, test, applyScalar = False):\n",
    "    print()\n",
    "    print('************** KCROSS VALIDATION AND LOGISTIC REGRESSION ***************')\n",
    "    print()\n",
    "    print('+++++++ FIND VALUE OF C +++++++')\n",
    "    print()\n",
    "\n",
    "    n = np.arange(-2,3)\n",
    "    r = pow(float(10),n)\n",
    "    bestCValue = 1.0\n",
    "    maxAccuracy = 0\n",
    "\n",
    "    kf = RepeatedKFold(n_splits = 3, n_repeats = 5, random_state = None) \n",
    "    for c in r:\n",
    "        maxScoreValues = []\n",
    "    \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index] \n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            if applyScalar:\n",
    "                scaler = RobustScaler().fit(X_train) #Scale cause of different variations in data set\n",
    "                X_train = scaler.transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "\n",
    "            lr = LogisticRegression(C = c).fit(X_train, y_train)\n",
    "            # print('\\n'\"Training Accuracy of L1 LogRess with C=%f:%f\" % (c, lr_l1.score(X_train,y_train)))\n",
    "            # print('\\n'\"Test Accuracy of L1 LogRegss with C=%f: %f\" % (c, lr_l1.score(X_test,y_test)))\n",
    "            maxScoreValues.append(lr.score(X_test, y_test))\n",
    "\n",
    "        average = sum(maxScoreValues)/len(maxScoreValues)\n",
    "        if average > maxAccuracy:\n",
    "            maxAccuracy = average\n",
    "            bestCValue = c\n",
    "\n",
    "    print()\n",
    "    print('+++++++ LOGISTIC +++++++')\n",
    "    print()\n",
    "\n",
    "    lr = LogisticRegression(C = bestCValue)\n",
    "    \n",
    "    if applyScalar:\n",
    "        X = RobustScaler().fit(X).transform(X)\n",
    "    \n",
    "    lr.fit(X, y)\n",
    "    calculateAccuracyAndConfusion(lr, X, y, 'Logistic Regression')\n",
    "    probability = lr.predict(test)\n",
    "    \n",
    "    return probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer SVM is a supervised machine learning algorithm which can be used for classification. In the implementation, the datasets along with model and parameters are passed to the bestParamCalculator function which returns the  best model and calculates the best accuracy.\n",
    "\n",
    "##### Parameters : \n",
    "- C: It is the inverse of regularization strength. Complexity increases with higher value of C and vice versa.\n",
    "- gammas: Gamma is a parameter for non linear hyper planes. Higher values of gamma tries to exactly fit the training data set.\n",
    "- kernel : It selects the type of hyperplane for seperating the data.\n",
    "- degrees: This parameter is used when the kernel is set to 'poly'.\n",
    "\n",
    "##### Disadvantages:\n",
    "- The probabilty estimates are not performed by multilayer SVM. Expensive five fold cross validation is used for getting the probabilty.\n",
    "- It is not suitable for large datasets.\n",
    "- In cases where dataset has more noise, multilayer SVM doesnt perform very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayerSVM(X, y, testDataset, applyScalar = True):\n",
    "    print('**************** SVM *******************')\n",
    "    param_grid = {\n",
    "         'C': [0.1, 1, 10, 100, 1000],\n",
    "         'gamma' : [0.001, 0.01, 0.1, 1],\n",
    "         'kernel' : ['linear','rbf', 'poly'],\n",
    "         'degree' : [0, 1, 2, 3, 4, 5, 6]\n",
    "              } \n",
    "\n",
    "    bestModel = bestParamCalculator(X, y, SVC(), param_grid, applyScalar)\n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(bestModel, X, y, 'Multilayer SVM')\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes classifier is a supervised classification model based on conditional probability which helps us to predict new instances based on the previous ones. We have implements two popular Naive Bayes algorithms, namely:\n",
    "- Multinomial Naive Bayes \n",
    "- Gaussian Naive Bayes\n",
    "\n",
    "##### Disadvantages: \n",
    "- Naive Bayes classifier makes a very strong assumption on the shape of your data distribution.\n",
    "- Data scarcity: For any possible value of a feature, you need to estimate a likelihood value by a frequentist approach.\n",
    "- Continuous features: It is common to use a binning procedure to make them discrete, but if you are not careful you can throw away a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial Naive Bayes classifier is used for classification with discrete features. MultinomialNB implements the naive Bayes algorithm for multinomially distributed data and normally requires integer feature counts.\n",
    "\n",
    "##### Parameters: \n",
    "- alpha: 'alpha' is a Smoothing parameter. 'alpha' >= 0 accounts for features not present in the learning samples and prevents    zero probabilities in further computations. Setting 'alpha' = 1 is called Laplace smoothing, while 'alpha' < 1 is called Lidstone smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multinomial(X, y, testDataset, applyNormalizer = False):\n",
    "    print()\n",
    "    print('**************** MULTINOMIAL NB *******************')\n",
    "    print()\n",
    "\n",
    "    n = np.arange(-2,3)\n",
    "    r = pow(float(10),n)\n",
    "\n",
    "    params = {\n",
    "        'alpha' : r\n",
    "    }\n",
    "\n",
    "    bestModel = bestParamCalculator(X, y, MultinomialNB(), params, applyNormalizer)\n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(bestModel, X, y, 'Multinomial NB')\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Classifier:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLPClassifier relies on an underlying Neural Network to perform the task of classification. This model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n",
    "\n",
    "##### Parameters:\n",
    "- hidden_layer_sizes : The ith element represents the number of neurons in the ith hidden layer.\n",
    "- activation : Activation function for the hidden layer. (‘tanh’: the hyperbolic tan function, returns f(x) = tanh(x). ‘relu’: the rectified linear unit function, returns f(x) = max(0, x))\n",
    "- solver : For weight optimization. (‘sgd’: refers to stochastic gradient descent. ‘adam’: refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba)\n",
    "- alpha : Regularization parameter.\n",
    "- learning_rate : Learning rate schedule for weight updates. (‘constant’: constant learning rate given by ‘learning_rate_init’. ‘adaptive’: keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing.)\n",
    "\n",
    "##### Disadvantages:\n",
    "- MLP with hidden layers have a non-convex loss function where there exists more than one local minimum. Therefore different random weight initializations can lead to different validation accuracy.\n",
    "- MLP requires tuning a number of hyperparameters such as the number of hidden neurons, layers, and iterations.\n",
    "- MLP is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpClassifier(X, y, testDataset, applyScalar = False):\n",
    "\n",
    "    print()\n",
    "    print('**************** MLP Classifier *******************')\n",
    "    print()\n",
    "\n",
    "    n = np.arange(-7,1)\n",
    "    r = pow(float(10),n)\n",
    "    params = {\n",
    "    #30 because roughly these many features we have and lastly confirming with default value\n",
    "    'hidden_layer_sizes': [(30,30,30), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': r,\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    }\n",
    "    bestModel = bestParamCalculator(X, y, MLPClassifier(max_iter=100), params, applyScalar)\n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(bestModel, X, y, 'MLPClassifier')\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: ML algorithms not covered in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "Gaussian Naive Bayes works on the features having continuous values which are normally distributed. \n",
    "\n",
    "##### Parameters: \n",
    "- var_smoothing: Portion of the largest variance of all the features that is added to variances for calculation of stability.\n",
    "  Default value for this parameter is (1e-09)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(X, y, testDataset, applyScalar = False):\n",
    "   \n",
    "    print()\n",
    "    print('**************** GAUSSIAN NB ****************')\n",
    "    print()\n",
    "\n",
    "    n = np.arange(-20,-5)\n",
    "    r = pow(float(10),n)\n",
    "    params = {\n",
    "        'var_smoothing' : r\n",
    "    }\n",
    "\n",
    "    model = GaussianNB()\n",
    "    bestModel = bestParamCalculator(X, y, model, params, applyScalar)\n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(model, X, y)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating the best model, the train data set, output of train data, test data set, model, parameters and preprocessing information is sent to a bestParamCalculator function which we have defined that will evaluate all the above passed arguements using Repeated K Fold function for the hyperparameter value. Using the best model, probabilty is then calculated by passing the test dataset to the predict function.\n",
    "\n",
    "##### Parameters:\n",
    "- n_estimators : It defines the number of trees in the forest.\n",
    "- max_features : It defines the maximum number of features considered for splitting the node.\n",
    "- max_depth : It is the maximum number of levels in each decision tree.\n",
    "- min_samples_split : minimum number of data points placed in a node before the node is split.\n",
    "\n",
    "##### Disadvantages : \n",
    "- Complexity : A lot of trees is created by this model and combines thier output. For evaluating this, it requires more computational power and resources.\n",
    "- Longer training period : As it generates a lot of trees, Random Forest requires more time to compute as compared to decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomForest(X, y, testDataset, applyScalar = False):\n",
    "    \n",
    "    print()\n",
    "    print('************ RANDOM FOREST WITH RANDOMIZED SEARCH ***************')\n",
    "    print()\n",
    "\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 5, stop = 200, num = 5)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n",
    "    min_samples_split = [5, 10]\n",
    "\n",
    "    random_grid = { \n",
    "                'n_estimators': n_estimators, \n",
    "                'max_features': max_features, \n",
    "                'max_depth': max_depth, \n",
    "                'min_samples_split': min_samples_split\n",
    "    }\n",
    "\n",
    "    bestModel = bestParamCalculator(X, y, RandomForestClassifier(), random_grid, applyScalar)\n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(bestModel, X, y, 'Random Forest')\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is a simple non-parametric classification technique. The number of neighbors, k, controls model flexibility and adjusts the biasvariance tradeoff.\n",
    "\n",
    "##### Parameters:\n",
    "- n_neighbors : Number of neighbors to use by default for kneighbors queries.\n",
    "- algorithm : Algorithm used to compute the nearest neighbors. (‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method. 'KDTree' used for fast generalized N-point problems) \n",
    "- weights : Used in prediction. (‘uniform’ : uniform weights. All points in each neighborhood are weighted equally. ‘distance’ : weight points by the inverse of their distance. In this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.)\n",
    "\n",
    "##### Disadvantages:\n",
    "- Does not work well with large dataset.\n",
    "- Does not work well with high dimensions.\n",
    "- Need feature scaling.\n",
    "- Sensitive to noisy data, missing values and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X, y, testDataset, applyScalar = False):\n",
    "    \n",
    "    print()\n",
    "    print('************ K NEAREST NEIGHBOURS ***************')\n",
    "    print()\n",
    "\n",
    "    random_grid = { \n",
    "                'n_neighbors': range(1,10), \n",
    "                'algorithm': ['auto', 'kd_tree'], \n",
    "                'weights' : ['distance', 'uniform']\n",
    "    }\n",
    "\n",
    "    bestModel = bestParamCalculator(X, y, KNeighborsClassifier(), random_grid, applyScalar)\n",
    "    \n",
    "    probability = bestModel.predict(testDataset)\n",
    "    calculateAccuracyAndConfusion(bestModel, X, y)\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eXtreme Gradient Boosting/ XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eXtreme Gradient Boosting or XGBoost is a library of gradient boosting algorithms optimized for modern data science problems. It utilizes the listed techniques with boosting and is bundled in a library that is simple to use. Some of the major advantages of XGBoost are that its very scalable/parallelizable, quick to execute, and typically out performs other algorithms.\n",
    "\n",
    "##### Parameters:\n",
    "- n_estimators: This is the number of trees you want to build before taking the maximum voting or averages of predictions. Higher number of trees gives you better performance but makes your code slower.\n",
    "- learning_rate: This determines the impact of each tree on the final outcome. Lower values are generally preferred as they make the model robust to the specific characteristics of tree.\n",
    "\n",
    "##### Disadvantages: \n",
    "- Depending on the learning rate of XGB the hyper parameters behave in a more robust way, along with the robustness it has a disadvantage of significant increase in the computation time whereas a significant jump can be obtained by feature engineering, creating ensembles of models which behaves in a slightly better manner as compared to time, for Example GBM (Gradient Boosting model). Also XGB requires careful tuning of hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyXGBoostFeatureSelection(X, y, testDataset, applyScalar = False):\n",
    "    print()\n",
    "    print('************ XG BOOST ***************')\n",
    "    print()\n",
    "    xgbClassifier = xgb.XGBClassifier(n_estimators=2000,learning_rate=0.3)\n",
    "    if applyScalar:\n",
    "        scaler = RobustScaler().fit(X) #Scale cause of different variations in data set\n",
    "        X = scaler.transform(X)    \n",
    "    xgbClassifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Main code for reading csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = pd.read_csv('train.csv')\n",
    "testDataset = pd.read_csv('test.csv')\n",
    "y = trainDataset['Category']\n",
    "X = trainDataset.iloc[:, :-1].values #without category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************** MAXIMUM LIKELIHOOD ***************\n",
      "\n",
      "Maximum likelihood  0    0.807569\n",
      "1    0.192431\n",
      "Name: Category, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "mle(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Calling various methods depending on algorithm we want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print('============== FINDING PREDICTIONS AND TRAINING ================')\n",
    "print()\n",
    "\n",
    "probabilityF = randomForest(X, y, testDataset, True)\n",
    "probabilityG = gaussian(X, y, testDataset, True)\n",
    "probabilityM = multinomial(X, y, testDataset)\n",
    "probabilityMC = mlpClassifier(X, y, testDataset, True)\n",
    "probabilityK = knn(X, y, testDataset, True)\n",
    "probabilityL = logistic(X, y, testDataset, True)\n",
    "probabilityA = applyXGBoostFeatureSelection(X, y, testDataset, True)\n",
    "probabilityMSVM = multilayerSVM(X, y, testDataset, True)\n",
    "\n",
    "showPerformancePlot()\n",
    "# generateCSV(probability, testDataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
